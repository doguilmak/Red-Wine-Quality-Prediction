# -*- coding: utf-8 -*-
"""red_wine_ann.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JD1wcbFOZWyfP2spxIijaPoDPfHK3kPo

**<h1 align=center><font size = 5>Red Wine Quality Prediction with Multi-Output Artificial Neural Networks</font></h1>**

<br>

<img src="https://learn.winecoolerdirect.com/wp-content/uploads/2015/07/red-wine.jpg" alt="wine">

<small>Picture Source:<a href="https://learn.winecoolerdirect.com/red-wine/"> Wine Cooler Direct</a></small>

<br>

<h2>Data Set Information:</h2>

<p>The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: <a href="https://www.vinhoverde.pt/en/">Web Link</a> or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.</p>

<br>

<h2>Attribute Information</h2>

<p>For more information, read [Cortez et al., 2009]. Input variables (based on physicochemical tests):</p>

<ol>
  <li>fixed acidity</li>
  <li>volatile acidity</li>
  <li>citric acid</li>
  <li>residual sugar</li>
  <li>chlorides</li>
  <li>free sulfur dioxide</li>
  <li>total sulfur dioxide</li>
  <li>density</li>
  <li>pH</li>
  <li>sulphates</li>
  <li>alcohol</li>
  <li><b>Output variable (based on sensory data): quality (score between 0 and 10)</b></li>
</ol>

<br>

<h2>Acknowledgements</h2>

<p>This dataset has been referred from <a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality">archive.ics.uci.edu</a>.</p>

<br>

<h2>Relevant Papers</h2>

<p>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties.
In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

Available at: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377?via%3Dihub">sciencedirect.com</a></p>

<br>

<h2>Keywords</h2>

<ul>
  <li>Wine</li>
  <li>Regression</li>
  <li>Machine Learning</li>
  <li>Quality Test</li>
</ul>

<br>

<h1>Objective for this Notebook</h1>

<p>Within the scope of this project, first we understand the Dataset & cleanup (if required). After that, we had tried to predict <code>quality</code> score based on independent variables.</p>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#importing_libraries">Importing Libraries</a></li>
<li><a href="https://#data_preprocessing">Data Preprocessing</a></li>
<li><a href="https://#build_fit_model">Build and Fit the Model</a></li>
<li><a href="https://#analize_model">Analize the Model</a></li>
<li><a href="https://#save_model">Saving the Model</a></li>

<br>

<p></p>
Estimated Time Needed: <strong>25 min</strong>
</div>

<br>

<a id="importing_libraries"></a>

<h2 align=center>Importing Libraries</h2>
"""

try:
  # %tensorflow_version only exists in Colab.
  get_ipython().run_line_magic('tensorflow_version', '2.x')
except Exception:
  pass

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
import itertools
import tensorflow as tf
import warnings
warnings.filterwarnings('ignore')

"""<br>

<a id="data_preprocessing"></a>

<h2 align=center>Data Preprocessing</h2>
"""

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv

df_red = pd.read_csv('/content/winequality-red.csv', sep=';')

df_red["is_red"] = 1

df_white = pd.read_csv('/content/winequality-white.csv', sep=';')

df_white["is_red"] = 0

"""<p>As we can see from the bottom, quality dataset is imbalanced. To solve this problem, we can make 3 different variables. Quality between 3-5 are low, 6-7 are mid, 8-9 are high quality. For low quality wines, we can call them as 0, for mid quality 1 and lastly for the good quality wines we can call them as 2.</p>"""

df_red['quality'].value_counts()

bins = [0, 5.5, 7.5, 10] # 3-5 are low, 6-7 are mid, 8-9 are high quality
labels = [0, 1, 2]
df_red['target'] = pd.cut(df_red['quality'], bins=bins, labels=labels)
df_white['target'] = pd.cut(df_white['quality'], bins=bins, labels=labels)

df_white["target"].unique()

df_red["target"].unique()

df_white.head(10)

df_red.head(10)

print("Number of NaN values: {} as red type.".format(df_red.isnull().sum().sum()))
print("Number of duplicated rows: {} as red type.".format(df_red.duplicated().sum()))
print("\nDropping duplicated rows...\n")

dp = df_red[df_red.duplicated(keep=False)]
df_red.drop_duplicates(inplace= True)
print("Number of duplicated rows: {}.".format(df_red.duplicated().sum()))

print("Number of NaN values: {} as white type.".format(df_white.isnull().sum().sum()))
print("Number of duplicated rows: {} as white type.".format(df_white.duplicated().sum()))
print("\nDropping duplicated rows...\n")

dp = df_white[df_white.duplicated(keep=False)]
df_white.drop_duplicates(inplace= True)
print("Number of duplicated rows: {}.".format(df_white.duplicated().sum()))

df = pd.concat([df_red, df_white], ignore_index=True)
df = df.iloc[np.random.permutation(len(df))] # Shuffle them
df.head(10)

df.describe().T

df.info()

plt.figure(figsize = (20, 10))
sns.set_style('whitegrid')
sns.histplot(data=df['target'], kde=True)
plt.title("Frequency of Quality Classes")
plt.xlabel("Quality Classes")
plt.ylabel("Frequency")

plt.figure(figsize = (20, 17))
sns.heatmap(df.corr(), annot=True)

sns.pairplot(df)
plt.show()

import matplotlib.patheffects as path_effects

fig = plt.figure(figsize = (20, 10), facecolor='w')
out_df=pd.DataFrame(df.groupby('target')['target'].count())

patches, texts, autotexts = plt.pie(out_df['target'], autopct='%1.1f%%',
                                    textprops={'color': "w"},
                                    startangle=90, shadow=True)

for patch in patches:
    patch.set_path_effects({path_effects.Stroke(linewidth=2.5,
                                                foreground='w')})
plt.title('Target Variable Distribution')
plt.legend(labels=["Low quality (0)", "Middle quality (1)", "Good quality (2)"], bbox_to_anchor=(1., .95), title="Quality")

for i in range(11):
  label = df.columns[i]
  plt.figure(figsize = (20, 10))
  plt.hist(df[df['target']==2][label], color='green', label="Good", 
           alpha=0.7, density=True, bins=15) # Good quality
  plt.hist(df[df['target']==1][label], color='orange', label="Mid", 
           alpha=0.7, density=True, bins=15) # Mid quality
  plt.hist(df[df['target']==0][label], color='blue', label="Bad", 
           alpha=0.7, density=True, bins=15) # Bad quality
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()

"""<br>

<h4>Test Train Split</h4>

<p>Creating train and test dataset Train/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. This will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the model. Therefore, it gives us a better understanding of how well our model generalizes on new data.

We know the outcome of each data point in the testing dataset, making it great to test with! Since this data has not been used to train the model, the model has no knowledge of the outcome of these data points. So, in essence, it is truly an out-of-sample testing. Let's split our dataset into train and test sets. Around 80% of the entire dataset will be used for training and 20% for testing.</p>

<br>
"""

train, test = train_test_split(df, test_size=0.2, random_state = 1)

train, val = train_test_split(train, test_size=0.2, random_state = 1)

train.shape

test.shape

val.shape

train_stats = train.describe().T
train_stats

train_stats = train.describe()
train_stats.pop('is_red')
train_stats = train_stats.transpose()

"""<p>We can't get std value of the target because it's <i>categorical</i>.</p>"""

train_stats

"""<p>Now, we need to seperate our data as dependent and independent variables. <code>target</code> and <code>is_red</code> columns are our independent variables. Columns other than this column are dependent variables.</p> """

# Function taken from 'Custom Models, Layers, and Loss Functions with TensorFlow' course.
# https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow

def format_output(data):
    is_red = data.pop('is_red')
    is_red = np.array(is_red)
    target = data.pop('target')
    target = np.array(target)
    return (target, is_red)

"""<p>Now, we can pop independent variables from <code>test</code>, <code>train</code> and <code>val</code> dataset."""

train_Y = format_output(train)

val_Y = format_output(val)

test_Y = format_output(test)

"""<p>For an instance, let's take a look at <code>train</code> dataframe:</p>"""

train.head()

"""<p>As we can see from dataframe, there aren't any <code>is_red</code> and <code>target</code> columns. Now, for our network, we should normalize our data with the following formula:</p>

$$x_{norm} = \frac{x - \mu}{\sigma}$$

"""

def norm(x):
    return (x - train_stats['mean']) / train_stats['std']

norm_train_X = norm(train)
    
norm_val_X = norm(val)
    
norm_test_X = norm(test)

norm_train_X.head()

"""<br>
<h2 align=center id="build_fit_model">Build and Fit the Model</h2>

"""

def base_model(inputs):
    
    x = tf.keras.layers.Dense(128, activation='relu', name='base_dense_1')(inputs)
    x = tf.keras.layers.Dense(128, activation='relu', name='base_dense_2')(x)
    x = tf.keras.layers.Dense(64, activation='relu', name='base_dense_3')(x)

    return x

def final_model(inputs):
    
    x = base_model(inputs)

    wine_quality = Dense(units='1', name='wine_quality')(x)
    wine_type = Dense(units='1', activation='sigmoid', name='wine_type')(x)

    model = Model(inputs=inputs, outputs=[wine_quality, wine_type])

    print(model.summary())
    return model

LR = 0.0001 #@param {type:"number"}
EPOCHS = 60 #@param {type:"number"}

inputs = tf.keras.layers.Input(shape=(12,))
rms = tf.keras.optimizers.RMSprop(lr=LR)
model = final_model(inputs)

from tensorflow.keras.utils import plot_model

plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')

model.compile(optimizer=rms, 
            loss = {'wine_type' : 'binary_crossentropy',
                  'wine_quality' : 'mean_squared_error'
                 },
            metrics = {'wine_type' : 'accuracy',
                     'wine_quality': tf.keras.metrics.RootMeanSquaredError()
                   }
            )

history = model.fit(norm_train_X, train_Y,
                    epochs = EPOCHS, validation_data=(norm_val_X, val_Y))

loss, wine_quality_loss, wine_type_loss, wine_quality_rmse, wine_type_accuracy = model.evaluate(x=norm_val_X, y=val_Y)

print(f'loss: {loss}')
print(f'wine_quality_loss: {wine_quality_loss}')
print(f'wine_type_loss: {wine_type_loss}')
print(f'wine_quality_rmse: {wine_quality_rmse}')
print(f'wine_type_accuracy: {wine_type_accuracy}')

"""<br>
<h2 align=center id="analize_model">Analize the Model</h2>
"""

predictions = model.predict(norm_test_X)
quality_pred = predictions[0]
type_pred = predictions[1]

print(quality_pred[0])

print(type_pred[0])

# Function taken from 'Custom Models, Layers, and Loss Functions with TensorFlow' course.
# https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow

def plot_metrics(metric_name, title, ylim=5):
    plt.figure(figsize = (20, 10))
    plt.title(title)
    plt.ylim(0,ylim)
    plt.plot(history.history[metric_name],color='blue',label=metric_name)
    plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)

# Function taken from 'Custom Models, Layers, and Loss Functions with TensorFlow' course.
# https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow

def plot_confusion_matrix(y_true, y_pred, title='', labels=[0,1]):
    cm = confusion_matrix(y_true, y_pred)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(cm)
    plt.title('Confusion matrix of the classifier')
    fig.colorbar(cax)
    ax.set_xticklabels([''] + labels)
    ax.set_yticklabels([''] + labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
          plt.text(j, i, format(cm[i, j], fmt),
                  horizontalalignment="center",
                  color="black" if cm[i, j] > thresh else "white")
    plt.show()

plot_confusion_matrix(test_Y[1], np.round(type_pred), title='Wine Type', labels = [0, 1])

plot_metrics('wine_quality_root_mean_squared_error', 'RMSE', ylim=2)

plot_metrics('wine_type_loss', 'Wine Type Loss', ylim=0.2)

"""<br>
<h2 align=center id="save_model">Saving the Model</h2>
"""

!mkdir -p saved_model
model.save('saved_model/my_model')

"""<p>Save as .h5 file</p>"""

model.save('my_model.h5')

"""<br>

<h1>Contact Me<h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")