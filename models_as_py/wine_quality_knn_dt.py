# -*- coding: utf-8 -*-
"""wine_quality_knn_dt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EtgLpGPk03qzgTY37Q3e2RRCzRuvgdd4

<h1 align=center><font size = 5>Wine Type Classification with K-NN</font></h1>

<br>

<img src="https://learn.winecoolerdirect.com/wp-content/uploads/2015/07/red-wine.jpg" alt="wine">

<small>Picture Source:<a href="https://learn.winecoolerdirect.com/red-wine/"> Wine Cooler Direct</a></small>

<br>

<h2>Data Set Information:</h2>

<p>The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: <a href="https://www.vinhoverde.pt/en/">Web Link</a> or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only <i>physicochemical</i> (inputs) and <i>sensory</i> (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as <i>classification</i> or <i>regression</i> tasks. The <i>classes</i> are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). <i>Outlier detection algorithms</i> could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.</p>

<br>

<h2>Acknowledgements</h2>

<p>This dataset has been referred from <a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality">archive.ics.uci.edu</a>.</p>

<br>

<h2>Relevant Papers</h2>

<p><b>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.</b> <i>Modeling wine preferences by data mining from physicochemical properties.
In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</i></p>

Available at: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377?via%3Dihub">sciencedirect.com</a></p>

<br>

<h2>Keywords</h2> 

<ul>
	<li>Wine</li>
	<li>Machine Learning</li>
	<li>Wine Type Classification</li>
	<li>K-Nearest Neighbors (K-NN)</li>
	<li>Decision Tree</li>
  <li>Grid Search</li>
</ul> 

<br>

<h1>Objective for this Notebook</h1>

<p>Within the scope of this project, first we understand the Dataset & cleanup (if required). After that, we had tried to build classification model to predict wine type with <i>K-Nearest Neighbors (K-NN)</i> and <i>Decision tree</i>.</p>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#importing_libraries">Importing Libraries</a></li>
<li><a href="https://#data_preprocessing">Data Preprocessing</a></li>
<li><a href="https://#classification">Classification</a></li>
<br>

<p></p>
Estimated Time Needed: <strong>20 min</strong>
</div>

<br>

<a id="importing_libraries"></a>

<h2 align=center>Importing Libraries</h2>
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import itertools
import pickle 
import seaborn as sns

from sklearn import metrics
from sklearn import preprocessing
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Function taken from 'Custom Models, Layers, and Loss Functions with TensorFlow' course.
# https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow

def plot_confusion_matrix(y_true, y_pred, title='', labels=[0,1]):
    cm = confusion_matrix(y_true, y_pred)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.grid(False)
    cax = ax.matshow(cm)
    plt.title(title)
    fig.colorbar(cax)
    ax.set_xticklabels([''] + labels)
    ax.set_yticklabels([''] + labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
          plt.text(j, i, format(cm[i, j], fmt),
                  horizontalalignment="center",
                  color="black" if cm[i, j] > thresh else "white")
    plt.show()

"""<br>

<a id="data_preprocessing"></a>

<h2 align=center>Data Preprocessing</h2>
"""

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv

df_red = pd.read_csv('/content/winequality-red.csv', sep=';')

df_white = pd.read_csv('/content/winequality-white.csv', sep=';')

"""<p>As we can see from the bottom, quality dataset is imbalanced. To solve this problem, we can make 3 different variables. Quality between 3-5 are low, 6-7 are mid, 8-9 are high quality. For low quality wines, we can call them as 0, for mid quality 1 and lastly for the good quality wines we can call them as 2.</p>"""

df_red['quality'].value_counts()

df_white['quality'].value_counts()

bins = [0, 5.5, 7.5, 10] # 3-5 are low(0), 6-7 are mid(1), 8-9 are high quality(2)
labels = [0, 1, 2]
df_red['quality'] = pd.cut(df_red['quality'], bins=bins, labels=labels)
df_white['quality'] = pd.cut(df_white['quality'], bins=bins, labels=labels)

df_white["is_red"] = 0

df_red["is_red"] = 1

"""<p>Now, we can concat our two dataframes. We can do it with following code:</p>"""

df = pd.concat([df_red, df_white], ignore_index=True)
df = df.iloc[np.random.permutation(len(df))] # Shuffle them
df.head(10)

df.shape

print("Number of NaN values: {}.".format(df.isnull().sum().sum()))
print("Number of duplicated rows: {}.".format(df.duplicated().sum()))
print("\nDropping duplicated rows...\n")

dp = df[df.duplicated(keep=False)]
df.drop_duplicates(inplace= True)
print("Number of duplicated rows: {}.".format(df.duplicated().sum()))

df.shape

df.describe().T

df.info()

plt.figure(figsize = (20, 10))
sns.set_style('whitegrid')
sns.histplot(data=df['is_red'], kde=True)
plt.title("Frequency of Type Classes (Red or White Wine)")
plt.xlabel("Type Classes")
plt.ylabel("Frequency")

plt.figure(figsize = (20, 17))
sns.heatmap(df.corr(), annot=True)

for i in range(11):
  label = df.columns[i]
  plt.figure(figsize = (20, 10))
  plt.hist(df[df['is_red']==1][label], color='red', label="Red Wine", 
           alpha=0.7, density=True, bins=15) # Red Wine
  plt.hist(df[df['is_red']==0][label], color='gray', label="White Wine", 
           alpha=0.7, density=True, bins=15) # White Wine
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()

"""<p>It's time to seperate our data as <i>dependent (Y)</i> and <i>independent(X)</i> variables.</p>"""

X = df[['fixed acidity', 'volatile acidity', 'citric acid',
       'residual sugar', 'chlorides', 'free sulfur dioxide',
       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']].values  #.astype(float)
X[0:5]

Y = df['is_red'].values
Y[0:5]

"""<p>Now, we have to normalize the data. Data Standardization gives the data zero mean and unit variance, it is good practice, especially for algorithms such as KNN which is based on the distance of data points:</p>"""

X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))
X[0:5]

"""<br>

<h4>Test Train Split</h4>

<p>Creating train and test dataset Train/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. This will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the model. Therefore, it gives us a better understanding of how well our model generalizes on new data.

We know the outcome of each data point in the testing dataset, making it great to test with! Since this data has not been used to train the model, the model has no knowledge of the outcome of these data points. So, in essence, it is truly an out-of-sample testing. Let's split our dataset into train and test sets. Around <i>80%</i> of the entire dataset will be used for training and <i>20%</i> for testing.</p>

"""

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=0)

"""<br>

<a id="classification"></a>

<h2 align=center>Classification</h2>

<p>In this section, we are going to build classification models <i>(K-NN and Decision Tree)</i> for predicting wine class (red or white).

<br>

<h3>K-NN Classification</h3>

<p>In <i>statistics</i>, the <i>k-nearest neighbors algorithm</i> is a <i>non-parametric supervised learning</i> method first developed by <i>Evelyn Fix</i> and <i>Joseph Hodges</i> in 1951, and later expanded by <i>Thomas Cover</i>. It is used for <i>classification</i> and <i>regression</i>. In both cases, the input consists of the <i>k closest training</i> examples in a data set. (<a href='https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm'>Wikipedia</a>)</p>
"""

from sklearn.neighbors import KNeighborsClassifier

k = 4
neigh = KNeighborsClassifier(n_neighbors=k).fit(x_train, y_train)
neigh

"""<p>Let's make some predictions!</p>"""

yhat = neigh.predict(x_test)
yhat[0:5]

"""<p>Now, let's take a look at our ground truth:</p>"""

y_test[0:5]

"""<p>It's time to build confusion matrix to see how strong our model.</p>"""

plot_confusion_matrix(y_test, yhat, title='K-NN Confusion Matrix', labels=[0, 1])

"""<h4>What about other K?</h4>

<p><i>K</i> in <i>K-NN</i>, is the number of nearest neighbors to examine. It is supposed to be specified by the user. So, how can we choose right value for <i>K</i>? The general solution is to reserve a part of your data for testing the accuracy of the model. We can calculate the accuracy of <i>K-NN</i> for different values of k.</p>
"""

Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))

for n in range(1, Ks):

    # Train Model and Predict
    neigh = KNeighborsClassifier(n_neighbors=n).fit(x_train, y_train)
    yhat = neigh.predict(x_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)

    std_acc[n-1] = np.std(yhat == y_test) / np.sqrt(yhat.shape[0])

mean_acc

plt.figure(figsize = (17, 10))
plt.plot(range(1, Ks), mean_acc, 'r')
plt.fill_between(range(1, Ks), mean_acc - 1 * std_acc, mean_acc + 1 * std_acc, alpha=0.10)
plt.fill_between(range(1, Ks), mean_acc - 3 * std_acc, mean_acc + 3 * std_acc, alpha=0.10, color="red")
plt.legend(('Accuracy ', '+/- 1xstd', '+/- 3xstd'))
plt.ylabel('Accuracy')
plt.xlabel('Number of Neighbors (K)')
plt.tight_layout()
plt.show()

print(f"The best accuracy was with {mean_acc.max()} \
with k = {mean_acc.argmax() + 1}")

"""<p>Save the model with best parameter.</p>"""

file_name = 'knn_model.sav'
pickle.dump(neigh, open(file_name, 'wb'))
 
# Load the K-NN Model
#loaded_model = pickle.load(open(file_name, 'rb'))

"""<br>

<h3>Decision Tree Classification</h3>

<p>A <i>decision tree</i> is a <i>decision support tool</i> that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. (<a href='https://en.wikipedia.org/wiki/Decision_tree'>Wikipedia</a>)</p>

<p>We will first create an instance of the <i>DecisionTreeClassifier</i> called <code>wineTree</code>.
Inside of the classifier, specify <code>criterion="entropy"</code> so we can see the information gain of each node.</p>
"""

from sklearn.tree import DecisionTreeClassifier

wineTree = DecisionTreeClassifier(criterion="entropy", max_depth=6)
wineTree

wineTree.fit(x_train, y_train)

"""<p>Let's make some predictions for the decision tree model!</p>"""

pred_tree = wineTree.predict(x_test)

print(pred_tree[0:5])

"""<p>Let's take a look at our ground truth for decision tree:</p>"""

print(y_test[0:5])

plot_confusion_matrix(y_test, pred_tree, title='Decision Tree Confusion Matrix', labels=[0, 1])

"""<p>We can use Grid Search method for finding the best parameters.</p>"""

from sklearn.model_selection import GridSearchCV

p = [{'criterion':['gini'],'splitter':['best'], 'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]},
     {'criterion':['entropy'] ,'splitter':['random'], 'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]},
     {'criterion':['gini'] ,'splitter':['random'], 'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]},
     {'criterion':['entropy'] ,'splitter':['best'], 'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]}]

gs = GridSearchCV(estimator=wineTree,
                  param_grid=p,
                  scoring='accuracy',
                  cv=5,
                  n_jobs=-1)

grid_search = gs.fit(x_train, y_train)
best_result = grid_search.best_score_
best_parameters = grid_search.best_params_
print("\nGrid Search")
print("Best result:\n", best_result)
print("Best parameters:\n", best_parameters)

"""<p>Save the model decision tree model.</p>"""

file_name = 'dt_model.sav'
pickle.dump(neigh, open(file_name, 'wb'))
 
# Load the Decision Tree Model
#loaded_model = pickle.load(open(file_name, 'rb'))

"""<br>

<h1>Contact Me<h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")