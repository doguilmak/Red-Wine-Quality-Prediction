# -*- coding: utf-8 -*-
"""red_wine_multiple_reg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZeXyne897Ubis8bsTcbmIjFStq31ZTRT

**<h1 align=center><font size = 5>Red Wine Multi-Class Regression</font></h1>**

<br>

<img src="https://learn.winecoolerdirect.com/wp-content/uploads/2015/07/red-wine.jpg" alt="wine">

<small>Picture Source:<a href="https://learn.winecoolerdirect.com/red-wine/"> Wine Cooler Direct</a></small>

<br>

<h2>Data Set Information:</h2>

<p>The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult: <a href="https://www.vinhoverde.pt/en/">Web Link</a> or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.</p>

<br>

<h2>Attribute Information</h2>

<p>For more information, read [Cortez et al., 2009]. Input variables (based on physicochemical tests):</p>

<ol>
  <li>fixed acidity</li>
  <li>volatile acidity</li>
  <li>citric acid</li>
  <li>residual sugar</li>
  <li>chlorides</li>
  <li>free sulfur dioxide</li>
  <li>total sulfur dioxide</li>
  <li>density</li>
  <li>pH</li>
  <li>sulphates</li>
  <li>alcohol</li>
  <li><b>Output variable (based on sensory data): quality (score between 0 and 10)</b></li>
</ol>

<br>

<h2>Acknowledgements</h2>

<p>This dataset has been referred from <a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality">archive.ics.uci.edu</a>.</p>

<br>

<h2>Relevant Papers</h2>

<p>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties.
In Decision Support Systems, Elsevier, 47(4):547-553, 2009.

Available at: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377?via%3Dihub">sciencedirect.com</a></p>

<br>

<h2>Keywords</h2>

<ul>
  <li>Wine</li>
  <li>Regression</li>
  <li>Machine Learning</li>
  <li>Quality Test</li>
</ul>

<br>

<h1>Objective for this Notebook</h1>

<p>Within the scope of this project, first we understand the Dataset & cleanup (if required). After that, we had tried to predict <code>quality</code> score based on independent variables.</p>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#importing_libraries">Importing Libraries</a></li>
<li><a href="https://#data_preprocessing">Data Preprocessing</a></li>
<li><a href="https://#multiple_regression">Multiple Regression</a></li>
<li><a href="https://#prediction">Making Predictions</a></li>
<li><a href="https://#save_and_load">Saving & Loading Model</a></li>

<br>

<p></p>
Estimated Time Needed: <strong>20 min</strong>
</div>

<br>

<a id="importing_libraries"></a>

<h2 align=center>Importing Libraries</h2>
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import sklearn
import warnings
warnings.filterwarnings('ignore')

sklearn.__version__

"""<br>

<a id="data_preprocessing"></a>

<h2 align=center>Data Preprocessing</h2>
"""

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv

df = pd.read_csv('winequality-red.csv', sep=';')

bins = [0, 5.5, 7.5, 10] # 3-5 are low, 6-7 are mid, 8-9 are high quality
labels = [0, 1, 2]
df['target'] = pd.cut(df['quality'], bins=bins, labels=labels)

df["target"].unique()

df.head(20)

df.tail()

df.shape

print("Number of NaN values: {}.".format(df.isnull().sum().sum()))

print("Number of duplicated rows: {}.".format(df.duplicated().sum()))

dp = df[df.duplicated(keep=False)]
dp.head(2)
df.drop_duplicates(inplace= True)
print("Number of duplicated rows: {}.".format(df.duplicated().sum()))

df.describe()

df.info()

plt.figure(figsize = (17, 17))
sns.heatmap(df.corr(), annot=True)

sns.pairplot(df)
plt.show()

import matplotlib.patheffects as path_effects

fig = plt.figure(figsize = (17, 10), facecolor='w')
out_df=pd.DataFrame(df.groupby('target')['target'].count())

patches, texts, autotexts = plt.pie(out_df['target'], autopct='%1.1f%%',
                                    textprops={'color': "w"},
                                    startangle=90, shadow=True)

for patch in patches:
    patch.set_path_effects({path_effects.Stroke(linewidth=2.5,
                                                foreground='w')})
plt.title('Target Variable Distribution')
plt.legend(labels=["Low quality (0)", "Middle quality (1)", "Good quality (2)"], bbox_to_anchor=(1., .95), title="Quality")

for i in range(11):
  label = df.columns[i]
  plt.hist(df[df['target']==2][label], color='green', label="Good", 
           alpha=0.7, density=True, bins=15) # Good quality
  plt.hist(df[df['target']==1][label], color='orange', label="Mid", 
           alpha=0.7, density=True, bins=15) # Mid quality
  plt.hist(df[df['target']==0][label], color='blue', label="Bad", 
           alpha=0.7, density=True, bins=15) # Bad quality
  plt.title(label)
  plt.ylabel("Probability")
  plt.xlabel(label)
  plt.legend()
  plt.show()

"""<br>

<h4>Test Train Split</h4>

<p>Creating train and test dataset Train/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. This will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the model. Therefore, it gives us a better understanding of how well our model generalizes on new data.

We know the outcome of each data point in the testing dataset, making it great to test with! Since this data has not been used to train the model, the model has no knowledge of the outcome of these data points. So, in essence, it is truly an out-of-sample testing. Let's split our dataset into train and test sets. Around 80% of the entire dataset will be used for training and 20% for testing. We create a mask to select random rows using the np.random.rand() function:</p>

<br>
"""

msk = np.random.rand(len(df)) < 0.8
train = df[msk]
test = df[~msk]

train.shape

test.shape

"""<br>

<a id="multiple_regression"></a>

<h2 align=center>Multiple Regression</h2>

<p>Multiple regression is a statistical technique that can be used to analyze the relationship between a single dependent variable and several independent variables. The objective of multiple regression analysis is to use the independent variables whose values are known to predict the value of the single dependent value.</p>

<br>

$$y_i = β_0 + β_1x_{i1} + β_2x_{i2} + ... + β_p x_{ip} + ϵ $$

<br>

$y_i$ = dependent variable

$x_i$ = independent variable

$β_0$ = y-intercept

$β_p$ = slope coefficients for each independent variable

$ϵ$ = error of the model (residuals)

<br>
"""

from sklearn import linear_model
regr = linear_model.LinearRegression()

X = train.drop("target", axis=1)
Y = train["target"]
regr.fit(X, Y)

print('Coefficients: ', regr.coef_)

print('Intercept: ', regr.intercept_)

X

Y

y_hat = regr.predict(test.drop("target", axis = 1))
x = np.asanyarray(test.drop("target", axis = 1))
y = np.asanyarray(test["target"])

x

y

# Commented out IPython magic to ensure Python compatibility.
print("Residual sum of squares: %.2f"
#       % np.mean((y_hat - y) ** 2))

print('Variance score: %.3f' % regr.score(x, y)) # 1 is perfect prediction

"""<br>

<a id="prediction"></a>

<h2 align=center>Making Predictions</h2>
"""

regr.predict(x[5:6])

print('Actual value: {}'.format(y[5:6]))

print('Error: {}'.format((regr.predict(x[5:6]) - y[5:6]) ** 2))

regr.predict(x[3:4])

print('Actual value: {}'.format(y[3:4]))

print('Error: {}'.format((regr.predict(x[3:4]) - y[3:4]) ** 2))

"""<br>

<a id="save_and_load"></a>

<h2 align=center>Saving & Loading Model</h2>
"""

import pickle
file = "regr.save"
pickle.dump(regr, open(file, 'wb'))

downloaded_data = pickle.load(open(file, 'rb'))
print(downloaded_data.predict(x))

"""<br>

<h1>Contact Me<h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")